---
layout: post
title: "Manipulation in Healthcare"
date: 2020-07-21
---

Institutional applications of algorithms raise questions about the privacy of public client data. This problem is particularly pronounced in healthcare. I got interested in this idea when I encountered an article entitled AI and Adversarial Attacks. You can read it here. 
AI and Adversarial Attacks presents a central challenge in the use of automated medical diagnostic systems in the public health sector: because machines are designed to a high degree of specificity, slight interferences with diagnostic algorithms can cause the best of “automated doctors” to declare false positive or false negative cases nearly 100% of the time. This is a consequence of the fact that humans are capable of interfering with the machines they create, implying if a medical professional sought to intentionally encourage a diagnostic system to declare a positive diagnosis, perhaps in hopes of deriving some gain from the pharmaceutical companies that would profit from the patient’s insurance company (or pocket),  it would be feasible for him/her to do so. Unfortunately, this is a significant trade-off to the expansion of diagnostic power that AI can have assuming its supervising medical professionals act ethically and responsibly (by not tampering with the system). Treatments benefit doctors who are interesting in making money, and if an algorithm can be manipulated to justify the need for treatments, dangerous concerns arise in terms of corporate motives and bioethics.  
Solutions to the aforementioned challenges are limited in scope mainly because manipulation of an AI system’s parameters is an inherent feature of its design. Central regulation of all publicly deployed diagnostic systems is a possibility, but issues arise in terms of operational cost and feasibility; it is expected to be nearly impossible to strictly monitor all medical institutions, especially those with high clinical output. Legal issues arise when considering the role of surveillance of medical institutions by a central authority. In addition, supervision of diagnostic systems may imply supervision of the data that are processed by those systems. The stakes are of various forms: how much regulation of AI is necessary to prevent fraud but reasonable enough to be respectful of developer rights, public privacy, and institutional access and operations.  
A technical approach to the problem would be to specifically tailor medical AI systems to resist manipulation. This would prevent the need for legal argument concerning regulation, but it would require significant advancement in backend development, which may not be foreseeable for many years. It is also likely that the cost of research and development required to engineer such “manipulation-proof” technology would outweigh the benefit of having algorithms that know what a fraudulent doctor looks like, since health and insurance fraud are difficult trends to localize and treat on their own. A more substantive fix may be to implement developer features that create a public system of diagnostic machines that work in tandem to deduce and study results from patient information; this way, the diagnosis for a particular patient is never made by a single algorithm and thus unlikely to be influenced by a single perpetrator. The role of distributed networking is key in this consideration, as linked AI’s across a public network would all have to be trained to respond to diverse scenarios and features.  
