---
layout: post
title: "My Take on Algorithm Transparency"
date: 2020-07-21
---

Public stakeholders of automated decision systems (ADS’s) may insist upon the right to explanation, which expresses the need for developers to provide concise and generating explanations for why their ADS’s do what they do. Several ethical concerns may justify this argument: that if developers cannot communicate the processes that automated deciders employ, it may be difficult to identify and penalize developers with ill intentions; that being able to have conversations with humans adds a dimension of trust to decision-making processes that should be preserved; and that convincing people to trust algorithms in sensitive social contexts, such as in the case of issuing loans to minority students for tuition, is too difficult. Expanding on the latter point, it can be said that the future of a generation of minority Americans could be heavily influenced by opportunities to study. Why should society trust a black box in a situation like this?  


If we allow automated decision-making protocols in social environments, it is reasonable to first ask that such protocols are comprehensible. If they were not, then ensuring they are appropriate and improving them may be intractable. An issue however may be that a purported method of algorithmic explanation is incapable of meeting very many criteria of comprehensibility. An argument is that due to this challenge, it is of interest to ban the use of automated systems in which social consequences apply.   


We argue to the contrary that the existence of multiple ways to explain an algorithm does not warrant an inconsideration of any one way to explain it. Moreover, the costs of an explanation are not always small enough to consistently warrant the demand for transparency. Those cases in which giving an explanation offers a dangerous amount of information, is not possible, or triggers legitimate concerns about intellectual property, are addressed. 


We may consider several cases in which the purpose of an ADS relies on public stakeholders’ ignorance of how the algorithm works. Cases of selection or admission are examples. If an algorithm is equipped to assess the qualifications of a set of subjects and select a subset based on this assessment, it may be of interest to not disclose criteria the algorithm uses. We would, by providing an explanation, be setting the algorithm up to be manipulated. If policymakers required Goldman Sachs to release a report on its ADS used in assessing entry-level analyst applicants, undergraduates may focus on certain sections of their resumes because “that’s what Goldman wants;” in effect, those individuals with stronger ability to interpret Goldman’s ADS may be more likely to secure the job. (This argument applies to the extent that the ADS Goldman uses reflects the qualities the firm looks for in new hires. But it is reasonable to assume that Goldman’s algorithms are not blindly choosing individuals based on irrelevant qualities).  When banks employ screeners to inform them which applicants should be given loans, because there is an assumed rhyme to their process, we have automated deciders that should more or less resemble black boxes to the public for the same reasons that Goldman does not tell students exactly how to prepare for interviews, write personal statements, and organize their resumes. 


Consider further the role of machine learning in automated decision procedures. A model is exposed to a large quantity of data from which it extracts patterns that inform future decisions. Unsurprisingly, a small amount of programming (excluding curation protocols) is required for ML-based ADS’s by virtue of the data being the guiding hand. The effect is that even if developers agreed on what constitutes sound algorithmic explanation, they could not provide it. A machine learning classifier used to predict recidivism of released convicts may make decisions due to the topology of the data it was fed; a policymaker who demands reasoning for its decisions would be requesting the developer to give the data. Presenting a 10,000-instance data set to the US Congress may not be feasible and informative for several obvious reasons. Focus on learning systems in ADS research offers new performance and cost insights. Frequent appearance of ADS’s created by people who know only as much as what the data looks like is highly likely. Take for example the use of ML for anomaly detection in MRI scans; computationally, it may be possible to identify tumors more quickly and accurately than can doctors. Asking the model’s developers “how it works” is a puzzling question. The model has drawn up the patterns it uses from the data it has seen, not because the developers chose to focus on certain features. It is not the case that patients are denied the right to explanation because they do not deserve it, but that it is not a right that can be issued. Banning the use of the model for this reason would disregard its potential in terms of optimizing MRI assessment, a hasty and uninformed measure.  


It then follows that there exist uses of ADS’s that should not or cannot involve the issuance of an explanation. Goldman Sachs may have an algorithm that works well to source talent from a diverse applicant pool; if we required explanation, certain individuals might figure out how to falsely write an application that reflects a minority background. The MRI tool may help public hospitals to diagnose patients quickly; if we required explanation, we would be requesting developers to communicate possibly inconveniently large datasets that offer little to policymakers.  Additionally, requiring a singular definition of algorithmic explanationPublic stakeholders of automated decision systems (ADS’s) may insist upon the right to explanation, which expresses the need for developers to provide concise and generating explanations for why their ADS’s do what they do. Several ethical concerns may justify this argument: that if developers cannot communicate the processes that automated deciders employ, it may be difficult to identify and penalize developers with ill intentions; that being able to have conversations with humans adds a dimension of trust to decision-making processes that should be preserved; and that convincing people to trust algorithms in sensitive social contexts, such as in the case of issuing loans to minority students for tuition, is too difficult. Expanding on the latter point, it can be said that the future of a generation of minority Americans could be heavily influenced by opportunities to study. Why should society trust a black box in a situation like this?  


If we allow automated decision-making protocols in social environments, it is reasonable to first ask that such protocols are comprehensible. If they were not, then ensuring they are appropriate and improving them may be intractable. An issue however may be that a purported method of algorithmic explanation is incapable of meeting very many criteria of comprehensibility. An argument is that due to this challenge, it is of interest to ban the use of automated systems in which social consequences apply.   
We argue to the contrary that the existence of multiple ways to explain an algorithm does not warrant an inconsideration of any one way to explain it. Moreover, the costs of an explanation are not always small enough to consistently warrant the demand for transparency. Those cases in which giving an explanation offers a dangerous amount of information, is not possible, or triggers legitimate concerns about intellectual property, are addressed. 


We may consider several cases in which the purpose of an ADS relies on public stakeholders’ ignorance of how the algorithm works. Cases of selection or admission are examples. If an algorithm is equipped to assess the qualifications of a set of subjects and select a subset based on this assessment, it may be of interest to not disclose criteria the algorithm uses. We would, by providing an explanation, be setting the algorithm up to be manipulated. If policymakers required Goldman Sachs to release a report on its ADS used in assessing entry-level analyst applicants, undergraduates may focus on certain sections of their resumes because “that’s what Goldman wants;” in effect, those individuals with stronger ability to interpret Goldman’s ADS may be more likely to secure the job. (This argument applies to the extent that the ADS Goldman uses reflects the qualities the firm looks for in new hires. But it is reasonable to assume that Goldman’s algorithms are not blindly choosing individuals based on irrelevant qualities).  When banks employ screeners to inform them which applicants should be given loans, because there is an assumed rhyme to their process, we have automated deciders that should more or less resemble black boxes to the public for the same reasons that Goldman does not tell students exactly how to prepare for interviews, write personal statements, and organize their resumes.  


Consider further the role of machine learning in automated decision procedures. A model is exposed to a large quantity of data from which it extracts patterns that inform future decisions. Unsurprisingly, a small amount of programming (excluding curation protocols) is required for ML-based ADS’s by virtue of the data being the guiding hand. The effect is that even if developers agreed on what constitutes sound algorithmic explanation, they could not provide it. A machine learning classifier used to predict recidivism of released convicts may make decisions due to the topology of the data it was fed; a policymaker who demands reasoning for its decisions would be requesting the developer to give the data. Presenting a 10,000-instance data set to the US Congress may not be feasible and informative for several obvious reasons. Focus on learning systems in ADS research offers new performance and cost insights. Frequent appearance of ADS’s created by people who know only as much as what the data looks like is highly likely. Take for example the use of ML for anomaly detection in MRI scans; computationally, it may be possible to identify tumors more quickly and accurately than can doctors. Asking the model’s developers “how it works” is a puzzling question. The model has drawn up the patterns it uses from the data it has seen, not because the developers chose to focus on certain features. It is not the case that patients are denied the right to explanation because they do not deserve it, but that it is not a right that can be issued. Banning the use of the model for this reason would disregard its potential in terms of optimizing MRI assessment, a hasty and uninformed measure.  


It then follows that there exist uses of ADS’s that should not or cannot involve the issuance of an explanation. Goldman Sachs may have an algorithm that works well to source talent from a diverse applicant pool; if we required explanation, certain individuals might figure out how to falsely write an application that reflects a minority background. The MRI tool may help public hospitals to diagnose patients quickly; if we required explanation, we would be requesting developers to communicate possibly inconveniently large datasets that offer little to policymakers.  Additionally, requiring a singular definition of algorithmic explanation may require imposing standards on all types of ADS’s. For example, if a recidivism prediction tool could be explained well by giving information about input feature space, policymakers may require that all algorithms be explained by giving information about input feature space. But it is possible that disclosing the input feature space of an algorithm used in, for example, path-planning systems for self-driving cars, would lead to trouble in intellectual property claims. Indeed, developers of the latter would be required to publish information that should be protected from other companies. There may very well exist a method to explain algorithms in self-driving cars that does not go so far as to disclose input feature space; requiring a unified notion of explanation however prevents us from applying different standards to different algorithms when such action may be necessary.  


We note from these examples the following: that algorithmic explanation is not uniformly constructive, may not be possible, and/or may introduce undesired legal friction. Banning ADS’s because algorithmic explanation cannot be achieved is equivalent to stating that explaining an ADS is always helpful, possible, and conducive to benefits that outweigh costs. We may consider an ADS that is well equipped to make fair decisions; but explaining it would encourage public stakeholders to “game” its performance. Algorithmic explanation is not always possible, as in the case of machine learning models. An implication along the lines of “if no explanation, then bad to use” does not exist in the case of automated decision systems.  


Given a situation in which our two options are ban an algorithm or allow it despite not having a simple explanation as to how it behaves, let us consider the latter. We ask if it is possible to give an explanation, and if an explanation should be given (if possible). If we make it that far, we then consider how many ways there are to give an explanation, and how developers should be required to present clear information to policymakers prior to deploying any automated system to the public space. If developers fail to meet these criteria, we then take the possibility of banning algorithms. Doing so before then would reflect impatience.  


I summarize in arguing that improving efforts of algorithmic explanation be considered in tandem with managing algorithmic transparency. It may be important to build auditing protocols in which the key features of an algorithm, including perhaps source code and annotations, are shared carefully. Rather than publish information about an algorithm’s performance – and potentially be at risk for gaming, manipulation, and legal costs – placing this information in developer-regulator pipelines may make for manageable and safe communication. Through initiatives such as explainable AI, it may also be possible for algorithms to report those inputs and features of greatest significance in the context of the observed decision. These solutions are not end-all, but their existence shows that to associate a poor explanation with a poor use for an algorithm is potentially unwise. We may require imposing standards on all types of ADS’s. For example, if a recidivism prediction tool could be explained well by giving information about input feature space, policymakers may require that all algorithms be explained by giving information about input feature space. But it is possible that disclosing the input feature space of an algorithm used in, for example, path-planning systems for self-driving cars, would lead to trouble in intellectual property claims. Indeed, developers of the latter would be required to publish information that should be protected from other companies. There may very well exist a method to explain algorithms in self-driving cars that does not go so far as to disclose input feature space; requiring a unified notion of explanation however prevents us from applying different standards to different algorithms when such action may be necessary.  


We note from these examples the following: that algorithmic explanation is not uniformly constructive, may not be possible, and/or may introduce undesired legal friction. Banning ADS’s because algorithmic explanation cannot be achieved is equivalent to stating that explaining an ADS is always helpful, possible, and conducive to benefits that outweigh costs. We may consider an ADS that is well equipped to make fair decisions; but explaining it would encourage public stakeholders to “game” its performance. Algorithmic explanation is not always possible, as in the case of machine learning models. An implication along the lines of “if no explanation, then bad to use” does not exist in the case of automated decision systems.  


Given a situation in which our two options are ban an algorithm or allow it despite not having a simple explanation as to how it behaves, let us consider the latter. We ask if it is possible to give an explanation, and if an explanation should be given (if possible). If we make it that far, we then consider how many ways there are to give an explanation, and how developers should be required to present clear information to policymakers prior to deploying any automated system to the public space. If developers fail to meet these criteria, we then take the possibility of banning algorithms. Doing so before then would reflect impatience.  
I summarize in arguing that improving efforts of algorithmic explanation be considered in tandem with managing algorithmic transparency. It may be important to build auditing protocols in which the key features of an algorithm, including perhaps source code and annotations, are shared carefully. Rather than publish information about an algorithm’s performance – and potentially be at risk for gaming, manipulation, and legal costs – placing this information in developer-regulator pipelines may make for manageable and safe communication. Through initiatives such as explainable AI, it may also be possible for algorithms to report those inputs and features of greatest significance in the context of the observed decision. These solutions are not end-all, but their existence shows that to associate a poor explanation with a poor use for an algorithm is potentially unwise. 
