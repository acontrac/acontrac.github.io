---
layout: post
title: "An Argument Against Debiasing on Autopilot Mode"
date: 2020-07-22
---

The example I'll use throughout this post is an applicant screening algorithm used by a financial firm to which a diverse pool of candidates applies. For example, Bank Bucks uses its "Alpha" system to assess the qualifications of the many candidates who applied in 2020. Alpha was trained by engineers using the profiles of perhaps 10,000 Bank Bucks candidate profiles in the last 10 years. Bank Bucks finds that the 90% of the candidates in 2020 who Alpha deems are capable are Asian males. 

This would be concerning to several people, most obviously those females and non-Asian males which Alpha is not representing well enough for Bank Bucks to claim on its website that it hires a diverse group of people on the floor. We might hear the claim, "Alpha is biased!" And Alpha is biased. Biased algorithms are as ubiquitous as are algorithms. That's the point of discussing challenges on the road to algorithmic fairness! 

Bias in algorithms comes from many contextual variables that I chose not to enumerate here (I don't know them). But one very common source is bias which exists in training data. For example, it might be the case that Bank Bucks has been applying high standards on candidate literacy in mathematics and statistics, and it may be that Asian males seeking employment in metropolitan areas are likely to be extremely fluent in these subjects, which would explain the tendency towards this group. There are simpler examples. If you have an algorithm that classifies apples as tasty or not depending on the apple's shape, color, and shine, and you pass it training data which overwhelmingly labels green apples as repugnant, the algorithm will be biased against green apples. 

Recently, there have been efforts to debias algorithms by attacking biases in training data. This is promising, because if datasets can be "cleaned" to train algorithms that ultimately more strongly satisfy fairness definitions, there is hope that some semblance of equal protection under the law and equality of opportunity are not threatened as in the case of Bank Bucks' Alpha. I will again choose to not enumerate the procedures of debiasing. Here is a <a href="https://arxiv.org/pdf/1908.10763.pdf">paper</a> if you are interested. 

The thought is that if automated decision systems are trained on debiased data, then selling them to companies that seek to use them for whatever they were going to use them for - applicant screening, for example - is reassuring. At first glance, it sounds reasonable because this way, companies can implement automated applicant  screening in a way that is more fair than if they used "junky" data. But just because data is biased does not mean it's junky. 

What if Bank Bucks had shared the same dataset used to train Alpha with a start-up that then designed a new screening system - call it Beta - trained on the original dataset subjected to a host of debiasing procedures? Let's assume we have some notion of fairness in mind (which Beta satisfies more strongly than does Alpha). Unless the start-up informed Bank Bucks that for the last ten years, Asian males have been hired at a significantly higher rate when compared to other groups, Bank Bucks might not reason that in fact, their standards for applicant fluency in mathematics and statistics may be too high for many minorities in their late twenties. In the immediate future, they might cast a wider net in terms of the demographics they reach in recruiting, but they might also march forward with the same technical rigor in mind that contributed to 10 years' worth of predominantly Asian male hiring, leading to a return to the same trend in the long run. Had Bank Bucks hired engineers to build a system in house, as they might do in the case of building something like Alpha, they might take note of Alpha's bias and ask if their historical hiring record reflects the same bias. This gives them more insight into why the bias exists, which is probably the most important question in the discussion.

Pitching this more formally, we can imagine that party A wants to purchase a debiased algorithm from party B; party A hands over its biased data and in return receives from party B a debiased algorithm that is then put to work for whatever party A needs to accomplish. The benefits of debiasing may be pronounced to different extents depending on the nature of the application in question, which is not what I'm concerned with. What is more insightful (in my opinion) is the danger of debiasing, because it is possible for party A to become blind to discriminatory behavior it has itself employed. Such dangers can be mitigated if there exists a pipeline beteen parties A and B through which the following question is asked and hopefully answered: assuming some fairness metric M is in mind, what does a Beta type algorithm illuminate about an Alpha type algorithm's inability to satisfy M? 

It might seem obvious that a pipeline like this should exist, because this is what happens when parties like A and B negotiate terms, sign contracts, and shake hands. But in a free market in which not every party is fluent enough in the language of machine learning to understand how debiasing works, perhaps it is possible that in the process of a transaction, party A doesn't ask how the Beta type algorithm is any better. It just asks for the Beta type algorithm. Or maybe party A does ask, but party B is concerned about party C learning about propietary debiasing strategies (enter the concern of intellectual property, which I'll save for another post) and so does not disclose anything meaningful about how the Beta type algorithm was made, only the fact that it is better than the Alpha type algorithm. We might imagine the increasing complexity of these problems as hundreds of parties become involved in several thousand transactions. There are many incentives, concerns, and nooks at which our question above can be ignored. Does that leave us helpless? No, but it should leave us wary of the fact that creating a market for debiased algorithms is not something we should move towards on autopilot. 

A possible point of contention to what I'm saying lies in the observation that maybe party A doesn't need to consult party B for a debiased algorithm - maybe party A can use licensed debiasing tools and do the work themselves. This is certainly possible, but if a nimble start-up has designed a debiasing tool that may be of interest to several buyers - be they financial firms, healthcare institutions, or universities - we have to keep in mind that the tool isn't a black-box type of accessory. It is tailored to curated data sets. The time and intensity of debiasing strategies will likely be significant enough to reason that for buyers who want to get their hands on legally compliant (which may not be synonymous to unbiased, but may in some sense be) algorithms, providing their data to the start-up is the easier way to go. I'm not saying that the predominant choice of all party As of the world will be to rely on a transaction with a party B in order to use unbiased algorithms. I'm saying in the case that these transactions happen privately and frequently, we need to perhaps move towards debiasing not as an end-all strategy but as an option that informs us about how to address human-driven biases that exist already. And that can't be done in full-fledged support for debiasing. 


